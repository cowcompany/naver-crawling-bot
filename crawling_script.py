import os
import requests
import csv
import datetime
import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

# ğŸ“‚ GitHub Actions í™˜ê²½ì—ì„œ ì €ì¥í•  í´ë”
DATA_FOLDER = "data"
os.makedirs(DATA_FOLDER, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ğŸ“Œ í¬ë¡¤ë§ ë‚ ì§œ ì¶”ê°€ (YYYY-MM-DD í˜•ì‹)
today_date = datetime.datetime.now().strftime("%Y-%m-%d")

def setup_selenium():
    """ Seleniumì„ ì´ìš©í•˜ì—¬ ë¡œê·¸ì¸ í›„ í¬ë¡¤ë§ """
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service("chromedriver.exe"), options=options)

    # ë„¤ì´ë²„ ë¶€ë™ì‚° í˜ì´ì§€ ì ‘ì†
    driver.get("https://new.land.naver.com/complexes/107024?ms=37.5617871,127.1826662,17&a=APT:PRE:ABYG:JGC&e=RETAIL")
    time.sleep(5)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°

    return driver

def crawl_lease_properties(page=1, max_retries=3):
    """ ì „ì„¸ ë§¤ë¬¼ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ (ì¿ í‚¤ ë° ì¬ì‹œë„ ê¸°ëŠ¥ í¬í•¨) """
    url = "https://new.land.naver.com/api/articles/complex/107024"
    params = {
        "realEstateType": "APT:PRE:ABYG:JGC",
        "tradeType": "B1",
        "page": page,
    }

    headers = {
        "accept": "application/json, text/plain, */*",
        "referer": "https://new.land.naver.com/",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    }

    # âœ… ë„¤ì´ë²„ ë¡œê·¸ì¸ ì¿ í‚¤ ì¶”ê°€ (ê°œë°œì ë„êµ¬ì—ì„œ ìµœì‹ ê°’ ë³µì‚¬)
    cookies = {
        "NNB": "VNWULNT7WWJGO",
        "NID_AUT": "rvd2DEVgotRe2lvOC6e6gF/MWxjIBoWSs/sRbrwX7ET9yLggY0njNhGcuqz5EgvN",
        "NID_JKL": "iTfl2S7lzH/EWgTBPsv1VKkH092eASi/AR9qBOQMQK0=",
    }

    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(url, params=params, headers=headers, cookies=cookies, timeout=10)
            print(f"ğŸ“¡ [DEBUG] í˜ì´ì§€ {page} ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}", flush=True)

            if response.status_code == 200:
                data = response.json()
                article_count = len(data.get("articleList", []))
                print(f"âœ… [DEBUG] í˜ì´ì§€ {page} ë°ì´í„° ê°œìˆ˜: {article_count}", flush=True)
                return data
            elif response.status_code in [401, 403, 504]:
                print(f"â³ HTTP ì˜¤ë¥˜ (í˜ì´ì§€ {page}, ì‹œë„ {attempt}/{max_retries}): {response.status_code}. ì¬ì‹œë„ ì¤‘...", flush=True)
                time.sleep(3)  # 3ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            else:
                print(f"âŒ HTTP ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {response.status_code}", flush=True)
                return None
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ (í˜ì´ì§€ {page}): {e}", flush=True)
            return None

    print(f"ğŸš¨ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼", flush=True)
    return None

# ğŸ“Œ ì „ì„¸ ê°€ê²©ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_price_to_number(price_str):
    """ '5ì–µ 8,000' â†’ 580000000, '6ì–µ' â†’ 600000000, '3ì–µ 5,000' â†’ 350000000 """
    if not price_str:
        return 0

    price_str = price_str.replace(",", "")
    match = re.match(r"(\d+)ì–µ(?:\s*(\d+))?", price_str)

    if match:
        billion = int(match.group(1)) * 100000000
        million = int(match.group(2)) * 10000 if match.group(2) else 0
        return billion + million
    return 0

def save_to_csv(article_list, filename):
    """ ì „ì„¸ ë§¤ë¬¼ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ """
    fieldnames = [
        "ë§¤ë¬¼ë²ˆí˜¸", "ë§¤ë¬¼ëª…", "ìœ í˜•", "ê±°ë˜ìœ í˜•", "ì „ì„¸ê°€", "ì „ì„¸ê°€(ìˆ«ì)",
        "ì „ìš©ë©´ì ", "ê³µê¸‰ë©´ì ", "ë°©í–¥", "ì¸µìˆ˜", "date"
    ]

    with open(filename, mode="w", newline="", encoding="utf-8-sig") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for article in article_list:
            price_str = article.get("dealOrWarrantPrc", "")
            price_num = convert_price_to_number(price_str)

            writer.writerow({
                "ë§¤ë¬¼ë²ˆí˜¸": article.get("articleNo", ""),
                "ë§¤ë¬¼ëª…": article.get("articleName", ""),
                "ìœ í˜•": article.get("realEstateTypeName", ""),
                "ê±°ë˜ìœ í˜•": article.get("tradeTypeName", ""),
                "ì „ì„¸ê°€": price_str,
                "ì „ì„¸ê°€(ìˆ«ì)": price_num,
                "ì „ìš©ë©´ì ": article.get("area1", ""),
                "ê³µê¸‰ë©´ì ": article.get("area2", ""),
                "ë°©í–¥": article.get("direction", ""),
                "ì¸µìˆ˜": article.get("floorInfo", ""),
                "date": today_date
            })

if __name__ == "__main__":
    print("ğŸš€ [DEBUG] í¬ë¡¤ë§ ì‹œì‘...", flush=True)
    
    # âœ… Seleniumì„ í†µí•œ ë¡œê·¸ì¸ í™•ì¸ (ì˜µì…˜)
    driver = setup_selenium()
    
    all_articles = []
    for page in range(1, 6):
        data = crawl_lease_properties(page)
        if data and "articleList" in data:
            all_articles.extend(data["articleList"])
        else:
            print(f"âŒ í˜ì´ì§€ {page}ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", flush=True)

    driver.quit()  # Selenium ë¸Œë¼ìš°ì € ì¢…ë£Œ

    filename = os.path.join(DATA_FOLDER, f"ì„¼í…€íŒ°ë¦¬ìŠ¤_ì „ì„¸_{today_date}.csv")

    if all_articles:
        save_to_csv(all_articles, filename)
        print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}", flush=True)
    else:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", flush=True)
